{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Awb3J_coge1j"
   },
   "source": [
    "#  k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmcNODm2ge12"
   },
   "source": [
    "Unsupervised machine learning models: clustering algorithms.\n",
    "Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
    "\n",
    "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as *k-means clustering*, which is implemented in ``sklearn.cluster.KMeans``.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13gguhR7ge2K"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6TpKAk9ge3L"
   },
   "source": [
    "## Introducing k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMbFk8Idge3a"
   },
   "source": [
    "The *k*-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
    "It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "\n",
    "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "Those two assumptions are the basis of the *k*-means model.\n",
    "We will soon dive into exactly *how* the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the *k*-means result.\n",
    "\n",
    "First, let's generate a two-dimensional dataset containing four distinct blobs.\n",
    "To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgREXerige3l",
    "outputId": "168859f8-e85a-48ce-fd06-7c5923d9f309"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jp34AOksge4Z"
   },
   "source": [
    "By eye, it is relatively easy to pick out the four clusters.\n",
    "The *k*-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B24i2USEge4q"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hD_943b8ge5c"
   },
   "source": [
    "Let's visualize the results by plotting the data colored by these labels.\n",
    "We will also plot the cluster centers as determined by the *k*-means estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mf1AF4GTge5q",
    "outputId": "af848760-e985-4d0b-b374-65c89ee7b6eb"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGgA_gzuge6M"
   },
   "source": [
    "The good news is that the *k*-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n",
    "But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly.\n",
    "Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to *k*-means involves an intuitive iterative approach known as *expectation–maximization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKN9EYMWge6X"
   },
   "source": [
    "## k-Means Algorithm: Expectation–Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWX4eQClge6j"
   },
   "source": [
    "Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science.\n",
    "*k*-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here.\n",
    "In short, the expectation–maximization approach here consists of the following procedure:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "   1. *E-Step*: assign points to the nearest cluster center\n",
    "   2. *M-Step*: set the cluster centers to the mean \n",
    "\n",
    "Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to.\n",
    "The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
    "\n",
    "The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.\n",
    "\n",
    "We can visualize the algorithm as shown in the following figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pb6iTzzge60"
   },
   "source": [
    "<img src=\"knc.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5m6T4nWGge7C"
   },
   "source": [
    "The *k*-Means algorithm is simple enough that we can write it in a few lines of code.\n",
    "The following is a very basic implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hrv_BFgIge7M",
    "outputId": "036fb4ad-ceab-4a4b-f6bd-fb517f009ca4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az4GaMjFge8L"
   },
   "source": [
    "#### The globally optimal result may not be achieved\n",
    "First, although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the *global* best solution.\n",
    "For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuYJiZDlge8a",
    "outputId": "9cea1c7e-9080-48dd-a611-7a0669e25dc5"
   },
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, 4, rseed=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLviKw7nge9H"
   },
   "source": [
    "Here the E–M approach has converged, but has not converged to a globally optimal configuration. For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the ``n_init`` parameter, which defaults to 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmfW93B-ge9V"
   },
   "source": [
    "#### The number of clusters must be selected beforehand\n",
    "Another common challenge with *k*-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.\n",
    "For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMrqeQ3yge9g",
    "outputId": "2ec44a51-5220-4201-fce7-f12d3685d25b"
   },
   "outputs": [],
   "source": [
    "labels = KMeans(6, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gpx_dckHge-O"
   },
   "source": [
    "#### k-means is limited to linear cluster boundaries\n",
    "The fundamental model assumptions of *k*-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "\n",
    "In particular, the boundaries between *k*-means clusters will always be linear, which means that it will fail for more complicated boundaries.\n",
    "Consider the following data, along with the cluster labels found by the typical *k*-means approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P4HyUsAge-b"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ff-WQeyqge--",
    "outputId": "f16c001b-8ca7-4b86-95fc-37240d87acfb"
   },
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKIwph8vge_j"
   },
   "source": [
    "One version of this kernelized *k*-means is implemented in Scikit-Learn within the ``SpectralClustering`` estimator.\n",
    "It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a *k*-means algorithm:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "05.11-K-Means.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
